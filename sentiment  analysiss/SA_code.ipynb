{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Parth rocks\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from numpy import array\n",
    "import tensorflow\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---review---\n",
      "[1, 2, 365, 1234, 5, 1156, 354, 11, 14, 2, 2, 7, 1016, 2, 2, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 2, 1117, 1831, 2, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 2, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 2, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 2, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
      "---label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"---review---\")\n",
    "print(x_train[6])\n",
    "print(\"---label---\")\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'and', 'and', 'br', 'villain', 'and', 'and', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'and', 'and', 'concept', 'issue', 'and', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'and', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'and', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'and', 'things', 'is', 'far', 'this', 'make', 'mistakes', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'and', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n"
     ]
    }
   ],
   "source": [
    "word2id=imdb.get_word_index()\n",
    "id2word= {i : word for word,i in word2id.items() }\n",
    "print([id2word.get(i,\" \")for i in x_train[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 2697\n"
     ]
    }
   ],
   "source": [
    "print('Maximum review length: {}'.format(\n",
    "len(max((x_train + x_test), key=len))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum review length: 70\n"
     ]
    }
   ],
   "source": [
    "print('Minimum review length: {}'.format(\n",
    "len(min((x_train + x_test), key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Parth rocks\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Parth rocks\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 32)           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213301 (833.21 KB)\n",
      "Trainable params: 213301 (833.21 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\Parth rocks\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Parth rocks\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "391/391 [==============================] - 510s 1s/step - loss: 0.5022 - accuracy: 0.7508 - val_loss: 0.3748 - val_accuracy: 0.8420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1cf12bdcc10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create the model\n",
    "max_review_length=500\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.01%\n"
     ]
    }
   ],
   "source": [
    "#Calculate Accuracy\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "0\n",
      "hello\n",
      "0.6970415\n"
     ]
    }
   ],
   "source": [
    "text=\"i hate it\"\n",
    "words = text.split() #split string into a list\n",
    "x = [[word2id[word] if (word in word2id and word2id[word]<=20000) else 0 for word in words]]\n",
    "x = sequence.pad_sequences(x, maxlen=500) # Should be same which you used for training data\n",
    "vector = np.array([x.flatten()])\n",
    "probability = model.predict(array([vector][0]))[0][0]\n",
    "class1 = np.argmax(model.predict(array([vector][0])), axis=-1)[0]\n",
    "print(class1)\n",
    "print('hello')\n",
    "print(probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main.py', 'SA_code.ipynb', 'trial.ipynb', 'tweet.py']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"E:\\Mehak Docs\\sentiment  analysiss/\")\n",
    "print(os. listdir('E:\\Mehak Docs\\sentiment  analysiss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Parth rocks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('E:\\Mehak Docs\\sentiment  analysiss/sentiment_analysis_model_new.h5')\n",
    "print(\"Saved model to disk\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "model = load_model('E:\\Mehak Docs\\sentiment  analysiss/sentiment_analysis_model_new.h5')\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "text=\" like\"\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment.polarity\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.312, 'pos': 0.688, 'compound': 0.296}\n",
      "positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Parth\n",
      "[nltk_data]     rocks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "text = \"its ok\"\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "score = sid.polarity_scores(text)\n",
    "print(score)\n",
    "if score['compound'] < 0:\n",
    "    print(\"negative\")\n",
    "elif (score['compound']==0 or (score['neu']>score['pos'])):\n",
    "    print(\"neutral\")\n",
    "else:\n",
    "    print(\"positive\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25fe6457d8a8cad1dcc4a5461cbab25919fcb0fef12527d211526cbdb48bdf61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
